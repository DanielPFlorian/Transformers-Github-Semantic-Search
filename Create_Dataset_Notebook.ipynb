{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "# Define the URL for fetching issues from the Hugging Face transformers repository\n",
    "url = \"https://api.github.com/repos/huggingface/transformers/issues?page=1&per_page=1\"\n",
    "\n",
    "# Send a GET request to the specified URL and store the response\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/transformers/issues/29225',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/transformers',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/transformers/issues/29225/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/transformers/issues/29225/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/transformers/issues/29225/events',\n",
       "  'html_url': 'https://github.com/huggingface/transformers/pull/29225',\n",
       "  'id': 2150031292,\n",
       "  'node_id': 'PR_kwDOCUB6oc5nsvBf',\n",
       "  'number': 29225,\n",
       "  'title': 'Fix `kwargs` handling in `generate_with_fallback`',\n",
       "  'user': {'login': 'cifkao',\n",
       "   'id': 8046580,\n",
       "   'node_id': 'MDQ6VXNlcjgwNDY1ODA=',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/8046580?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/cifkao',\n",
       "   'html_url': 'https://github.com/cifkao',\n",
       "   'followers_url': 'https://api.github.com/users/cifkao/followers',\n",
       "   'following_url': 'https://api.github.com/users/cifkao/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/cifkao/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/cifkao/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/cifkao/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/cifkao/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/cifkao/repos',\n",
       "   'events_url': 'https://api.github.com/users/cifkao/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/cifkao/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 0,\n",
       "  'created_at': '2024-02-22T22:14:37Z',\n",
       "  'updated_at': '2024-02-22T22:40:37Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'NONE',\n",
       "  'active_lock_reason': None,\n",
       "  'draft': False,\n",
       "  'pull_request': {'url': 'https://api.github.com/repos/huggingface/transformers/pulls/29225',\n",
       "   'html_url': 'https://github.com/huggingface/transformers/pull/29225',\n",
       "   'diff_url': 'https://github.com/huggingface/transformers/pull/29225.diff',\n",
       "   'patch_url': 'https://github.com/huggingface/transformers/pull/29225.patch',\n",
       "   'merged_at': None},\n",
       "  'body': '# What does this PR do?\\r\\n\\r\\nI found a bug ([here](https://github.com/huggingface/transformers/blob/dabe8556686a5727f7b707099967c8ce8ff16e96/src/transformers/models/whisper/generation_whisper.py#L776)) where the `kwargs` argument to `generate_with_fallback()` in `WhisperGenerationMixin` is modified in-place by `kwargs.pop(\"num_beams\", 1)`, which results in the following problems:\\r\\n1. Subsequent iterations of the fallback loop will use `num_beams=1` (although this has no effect provided that temperature 0 appears only once in the temperature list).\\r\\n2. More importantly, since `kwargs` is passed by reference (and not via the `**kwargs` syntax), modifications to it propagate to the caller and to subsequent calls of `generate_with_fallback`, so **every chunk except for the first one will always use `num_beams=1`**.\\r\\n\\r\\nThis PR:\\r\\n1. changes the `pop()` to `get()` to avoid modifying `kwargs` between loop iterations,\\r\\n2. makes sure a copy of `kwargs` is made as the first step in `generate_with_fallback()` to prevent any changes to it from propagating outside the method call.\\r\\n3. makes sure the keys that were assigned to `generation_config` are removed from the keyword arguments to `super().generate()` (to avoid overriding the former), but this is done in a copy of `kwargs` that is not reused between iterations.\\r\\n\\r\\n<!--\\r\\nCongratulations! You\\'ve made it this far! You\\'re not quite done yet though.\\r\\n\\r\\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it\\'s a great title that fully reflects the extent of your awesome contribution.\\r\\n\\r\\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\\r\\n\\r\\nOnce you\\'re done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don\\'t hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\\r\\n-->\\r\\n\\r\\n\\r\\n## Before submitting\\r\\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that\\'s the case).\\r\\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request), Pull Request section?\\r\\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link to it if that\\'s the case.\\r\\n- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\\r\\n- [ ] Did you write any new necessary tests?\\r\\n\\r\\n\\r\\n## Who can review?\\r\\n\\r\\n@patrickvonplaten @sanchit-gandhi @ylacombe \\r\\n',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/transformers/issues/29225/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/transformers/issues/29225/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the JSON response from the API to see what the response looks like\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub token for authorization\n",
    "GITHUB_TOKEN = ''  # Copy your GitHub token here\n",
    "\n",
    "# Headers for API request\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that can download all the issues from a GitHub repository.\n",
    "# 5,000 requests per hour is the GitHub rate limit for users\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"transformers\",\n",
    "    num_issues=30000,\n",
    "    rate_limit=5000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = (\n",
    "        pd.DataFrame.from_records(all_issues)\n",
    "        .dropna(axis=1, how=\"all\")\n",
    "    )\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your internet connection and the size of the dataset, this can take several minutes to hours to run...\n",
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'state_reason', 'draft', 'pull_request'],\n",
      "    num_rows: 28908\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Once the issues are downloaded we can load them locally\n",
    "\n",
    "issues_dataset = datasets.load_dataset(\n",
    "    \"json\", data_files=\"transformers-issues.jsonl\", split=\"train\"\n",
    ")\n",
    "print(issues_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/transformers/issues/28316\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/transformers/pull/5941\n",
      ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/transformers/pulls/5941', 'html_url': 'https://github.com/huggingface/transformers/pull/5941', 'diff_url': 'https://github.com/huggingface/transformers/pull/5941.diff', 'patch_url': 'https://github.com/huggingface/transformers/pull/5941.patch', 'merged_at': None}\n",
      "\n",
      ">> URL: https://github.com/huggingface/transformers/pull/17329\n",
      ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/transformers/pulls/17329', 'html_url': 'https://github.com/huggingface/transformers/pull/17329', 'diff_url': 'https://github.com/huggingface/transformers/pull/17329.diff', 'patch_url': 'https://github.com/huggingface/transformers/pull/17329.patch', 'merged_at': datetime.datetime(2022, 5, 18, 17, 7, 49)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = issues_dataset.shuffle(seed=777).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the dataset to add a new key-value pair indicating whether the issue is a pull request\n",
    "issues_dataset = issues_dataset.map(\n",
    "    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 13320\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataset to include only closed issues that are not pull requests\n",
    "filtered_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and x[\"state\"] == \"closed\")\n",
    ")\n",
    "# Print the filtered dataset\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> created_at: 2023-07-26 14:42:05\n",
      ">> closed_at: 2023-08-01 09:53:30\n",
      "\n",
      ">> created_at: 2023-01-13 14:12:37\n",
      ">> closed_at: 2023-01-16 10:00:32\n",
      "\n",
      ">> created_at: 2020-04-17 21:35:44\n",
      ">> closed_at: 2020-08-27 05:17:28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataset with a specific seed and select a range of columns\n",
    "sample = filtered_dataset.shuffle(seed=777).select(range(3))\n",
    "\n",
    "# Print out a sample of the the created_at and closed_at entries\n",
    "for created, closed in zip(\n",
    "    sample[\"created_at\"], sample[\"closed_at\"]\n",
    "):\n",
    "    print(f\">> created_at: {created}\")\n",
    "    print(f\">> closed_at: {closed}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> created_at: 2023-07-26 14:42:05\n",
      ">> closed_at: 2023-08-01 09:53:30\n",
      ">> time_to_close: 5 days, 19:11:25\n",
      "\n",
      ">> created_at: 2023-01-13 14:12:37\n",
      ">> closed_at: 2023-01-16 10:00:32\n",
      ">> time_to_close: 2 days, 19:47:55\n",
      "\n",
      ">> created_at: 2020-04-17 21:35:44\n",
      ">> closed_at: 2020-08-27 05:17:28\n",
      ">> time_to_close: 131 days, 7:41:44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Map the dataset to calculate the time taken to close\n",
    "filtered_dataset = filtered_dataset.map(\n",
    "    lambda x: {\"time_to_close\": x[\"closed_at\"] -x[\"created_at\"]}\n",
    ")\n",
    "\n",
    "# Shuffle the dataset and select a sample of 3\n",
    "sample = filtered_dataset.shuffle(seed=777).select(range(3))\n",
    "\n",
    "# Iterate through the sample and print the created_at, closed_at, and time_to_close\n",
    "for created, closed, time_to_close in zip(\n",
    "    sample[\"created_at\"], sample[\"closed_at\"], sample[\"time_to_close\"]\n",
    "):\n",
    "    print(f\">> created_at: {created}\")\n",
    "    print(f\">> closed_at: {closed}\")\n",
    "    print(f\">> time_to_close: {time_to_close}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time to close issues in Transformers library that are not pull requests: 38 days 20:03:59.734009\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of the \"time_to_close\" column in the filtered dataset\n",
    "mean = filtered_dataset.with_format(\"pandas\")[\"time_to_close\"].mean()\n",
    "\n",
    "# Print the calculated mean\n",
    "print(f'Average time to close issues in Transformers library that are not pull requests: {mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> created_at: 2021-07-15 13:18:53\n",
      ">> closed_at: 2021-07-15 15:40:17\n",
      ">> time_to_close_pr: 2:21:24\n",
      "\n",
      ">> created_at: 2021-06-09 17:19:10\n",
      ">> closed_at: 2021-06-09 17:49:27\n",
      ">> time_to_close_pr: 0:30:17\n",
      "\n",
      ">> created_at: 2019-12-10 13:53:28\n",
      ">> closed_at: 2019-12-10 14:13:34\n",
      ">> time_to_close_pr: 0:20:06\n",
      "\n",
      "Average time to close Pull Requests in Transformers library: 10 days 17:56:28.009275\n"
     ]
    }
   ],
   "source": [
    "# Filter for closed pull requests\n",
    "pr_filtered_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == True and x[\"state\"] == \"closed\")\n",
    ")\n",
    "# Calculate time taken to close pull requests\n",
    "pr_filtered_dataset = pr_filtered_dataset.map(\n",
    "    lambda x: {\"time_to_close_pr\": x[\"closed_at\"] - x[\"created_at\"]}\n",
    ")\n",
    "# Select a sample of 3 records and shuffle\n",
    "sample = pr_filtered_dataset.shuffle(seed=777).select(range(3))\n",
    "\n",
    "# Print information for each record in the sample\n",
    "for created, closed, time_to_close_pr in zip(\n",
    "    sample[\"created_at\"], sample[\"closed_at\"], sample[\"time_to_close_pr\"]\n",
    "):\n",
    "    print(f\">> created_at: {created}\")\n",
    "    print(f\">> closed_at: {closed}\")\n",
    "    print(f\">> time_to_close_pr: {time_to_close_pr}\\n\")\n",
    "\n",
    "# Calculate mean time to close pull requests\n",
    "mean = pr_filtered_dataset.with_format(\"pandas\")[\"time_to_close_pr\"].mean()\n",
    "\n",
    "# Print the mean time to close pull requests\n",
    "print(f\"Average time to close Pull Requests in Transformers library: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmenting the dataset with issue comments  \n",
    "# 5,000 requests per hour is the GitHub rate limit for users\n",
    "\n",
    "def get_comments(issue_number):\n",
    "    url = (f\"https://api.github.com/repos/huggingface/transformers/issues/{issue_number}/comments\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return [r[\"body\"] for r in response.json()]\n",
    "    except:\n",
    "        print(f\"Error generating comments for issue {issue_number}.\\nResponse:\\n {response.text}\")\n",
    "        print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "        time.sleep(60 * 60 + 1)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the issues dataset to include comments for each issue\n",
    "issues_with_comments_dataset = issues_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments((x[\"number\"]))}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'issues_with_comments_dataset' DataFrame to JSON format and save it to a file\n",
    "# with each record as a separate line\n",
    "issues_with_comments_dataset.to_json(\n",
    "    \"transformers-issues-with-comments.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
      "    num_rows: 28908\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Test loading the dataset from the json file\n",
    "transformers_issues_w_comments = datasets.load_dataset(\n",
    "    \"json\", data_files=\"transformerst-issues-with-comments.jsonl\", split=\"train\"\n",
    ")\n",
    "\n",
    "print(issues_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to the Hugging Face to push dataset to hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe68d09cd9f4db2a7cfd4507dfcca05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9742efd5d95426db016dee0effa4b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/29 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/DanielPFlorian/Transformers-Github-Issues/commit/883eacd7d4253bf6bd815e0471097ae474bc1900', commit_message='Upload dataset', commit_description='', oid='883eacd7d4253bf6bd815e0471097ae474bc1900', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to the Hugging Face hub\n",
    "transformers_issues_w_comments.push_to_hub(\n",
    "    \"DanielPFlorian/Transformers-Github-Issues\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f070b2b9970450d949f56c91805adda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1066c5edce94f05937abce1f90b9d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce8b263df3d4aadadc75b79325a0cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/28908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'state_reason', 'draft', 'pull_request'],\n",
       "    num_rows: 28908\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test loading the dataset from the Hugging Face hub\n",
    "remote_dataset = datasets.load_dataset(\"DanielPFlorian/transformers-github-issues\", split=\"train\")\n",
    "remote_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

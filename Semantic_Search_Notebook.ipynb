{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "issues_dataset = datasets.load_dataset(\n",
    "    \"json\", data_files=\"transformers-issues-with-comments.jsonl\", split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the dataset to set the \"is_pull_request\" field based on whether the issue is a pull request or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 28908\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.map(\n",
    "    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering for non-pull request issues with comments as these are most helpful for user queries and will introduce noise in the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 13447\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of columns in our dataset, most of which we don’t need to build our search engine. We will keep the most informative columns and drop the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 13447\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the column names from the dataset\n",
    "columns = issues_dataset.column_names\n",
    "\n",
    "# Define the columns to keep in the dataset\n",
    "columns_to_keep = [\"title\", \"body\",\"html_url\", \"comments\"]\n",
    "\n",
    "# Find the columns to remove from the dataset\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "\n",
    "# Remove the columns from the dataset\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# Display the updated dataset\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our embeddings we’ll augment each comment with the issue’s title and body, since these fields often include useful contextual information. Because our comments column is currently a list of comments for each issue, we need to “explode” the column so that each row consists of an (html_url, title, body, comment) tuple. In Pandas we can do this with the DataFrame.explode() function, which creates a new row for each element in a list-like column, while replicating all the other column values. To see this in action, let’s first switch to the Pandas DataFrame format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format of the issues_dataset to \"pandas\"\n",
    "issues_dataset.set_format(\"pandas\")\n",
    "# Create a new dataframe df by copying the entire issues_dataset\n",
    "df = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df.dropna(axis=0,how = \"any\",inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the first row in this DataFrame we can see there are four comments associated with this issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi @arda1906, thanks for raising an issue!\\r\\n\\r\\nWithout more information about the error i.e. what does it mean to \"not work\" and what is the expected behaviour? we won\\'t be able to help you.  \\r\\n\\r\\nFrom the snippet, it\\'s not entirely clear how the code is being run, but there are two separate commands which should be entered on separate lines or cells\\r\\n\\r\\n```py\\r\\nfrom huggingface_hub import notebook_login\\r\\n\\r\\nnotebook_login()\\r\\n```',\n",
       " 'hi,I am giving details\\r\\n> I am trying this code to train the model\\r\\n\\r\\n>```python\\r\\n>trainer = Trainer(model=model,args=training_args,\\r\\n                 compute_metrics=compute_metrics,\\r\\n                 train_dataset=emotion_encoded[\"train\"],\\r\\n                 eval_dataset=emotion_encoded[\"validation\"],\\r\\n                 tokenizer=tokenizer)\\r\\ntrainer.train()\\r\\n\\r\\n>and I am facing this error:\\r\\n>LocalTokenNotFoundError: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.\\r\\n>I have thought to apply the my token in the jupyter notebook like this:\\r\\n>```\\r\\n> ```python\\r\\n> from huggingface_hub import notebook_login\\r\\n> \\r\\n> notebook_login()\\r\\n>\\r\\n> ```\\r\\n>help me please:(\\r\\n',\n",
       " 'Hi @arda1906, are you running the notebook login cells before calling Trainer? Are you passing in a token to the interactive text box that appears when running notebook_login? ',\n",
       " '> Hi @arda1906, are you running the notebook login cells before calling Trainer? Are you passing in a token to the interactive text box that appears when running notebook_login?\\r\\n\\r\\n![20240221_210517](https://github.com/huggingface/transformers/assets/157398066/57ee7c71-1614-4c44-8dc2-144f47cafacd)\\r\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we explode df, we expect to get one row for each of these comments. Let’s check if that’s the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/transformers/is...</td>\n",
       "      <td>To enter token in jupyter notebook issue</td>\n",
       "      <td>Hi @arda1906, thanks for raising an issue!\\r\\n...</td>\n",
       "      <td>I run this [from huggingface_hub import notebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/transformers/is...</td>\n",
       "      <td>To enter token in jupyter notebook issue</td>\n",
       "      <td>hi,I am giving details\\r\\n&gt; I am trying this c...</td>\n",
       "      <td>I run this [from huggingface_hub import notebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/transformers/is...</td>\n",
       "      <td>To enter token in jupyter notebook issue</td>\n",
       "      <td>Hi @arda1906, are you running the notebook log...</td>\n",
       "      <td>I run this [from huggingface_hub import notebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/transformers/is...</td>\n",
       "      <td>To enter token in jupyter notebook issue</td>\n",
       "      <td>&gt; Hi @arda1906, are you running the notebook l...</td>\n",
       "      <td>I run this [from huggingface_hub import notebo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/transformers/is...   \n",
       "1  https://github.com/huggingface/transformers/is...   \n",
       "2  https://github.com/huggingface/transformers/is...   \n",
       "3  https://github.com/huggingface/transformers/is...   \n",
       "\n",
       "                                      title  \\\n",
       "0  To enter token in jupyter notebook issue   \n",
       "1  To enter token in jupyter notebook issue   \n",
       "2  To enter token in jupyter notebook issue   \n",
       "3  To enter token in jupyter notebook issue   \n",
       "\n",
       "                                            comments  \\\n",
       "0  Hi @arda1906, thanks for raising an issue!\\r\\n...   \n",
       "1  hi,I am giving details\\r\\n> I am trying this c...   \n",
       "2  Hi @arda1906, are you running the notebook log...   \n",
       "3  > Hi @arda1906, are you running the notebook l...   \n",
       "\n",
       "                                                body  \n",
       "0  I run this [from huggingface_hub import notebo...  \n",
       "1  I run this [from huggingface_hub import notebo...  \n",
       "2  I run this [from huggingface_hub import notebo...  \n",
       "3  I run this [from huggingface_hub import notebo...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it worked. Now that we’re finished with Pandas, we can quickly switch back to a Dataset by loading the DataFrame in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 62546\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = datasets.Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have one comment per row, let’s create a new comments_length column that contains the number of words per comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 62546/62546 [00:03<00:00, 18942.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this new column to filter out short comments, which typically include things like “cc @lewtun” or “Thanks!” that are not relevant for our search engine. There’s no precise number to select for the filter, but around 15 words seems like a good start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 62546/62546 [00:00<00:00, 146027.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 46959\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cleaned up our dataset a bit, let’s concatenate the issue title, description, and comments together in a new text column. We’ll write a simple function that we can pass to Dataset.map():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46959/46959 [00:05<00:00, 8300.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    # Concatenate title, body, and comments\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our new text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/transformers/issues/29161',\n",
       " 'title': 'To enter token in jupyter notebook issue',\n",
       " 'comments': 'hi,I am giving details\\r\\n> I am trying this code to train the model\\r\\n\\r\\n>```python\\r\\n>trainer = Trainer(model=model,args=training_args,\\r\\n                 compute_metrics=compute_metrics,\\r\\n                 train_dataset=emotion_encoded[\"train\"],\\r\\n                 eval_dataset=emotion_encoded[\"validation\"],\\r\\n                 tokenizer=tokenizer)\\r\\ntrainer.train()\\r\\n\\r\\n>and I am facing this error:\\r\\n>LocalTokenNotFoundError: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.\\r\\n>I have thought to apply the my token in the jupyter notebook like this:\\r\\n>```\\r\\n> ```python\\r\\n> from huggingface_hub import notebook_login\\r\\n> \\r\\n> notebook_login()\\r\\n>\\r\\n> ```\\r\\n>help me please:(\\r\\n',\n",
       " 'body': \"I run this [from huggingface_hub import notebook_login\\r\\nnotebook_login() ] on cell and enter my token. but it doesn't work:(\",\n",
       " 'comment_length': 89,\n",
       " 'text': 'To enter token in jupyter notebook issue \\n I run this [from huggingface_hub import notebook_login\\r\\nnotebook_login() ] on cell and enter my token. but it doesn\\'t work:( \\n hi,I am giving details\\r\\n> I am trying this code to train the model\\r\\n\\r\\n>```python\\r\\n>trainer = Trainer(model=model,args=training_args,\\r\\n                 compute_metrics=compute_metrics,\\r\\n                 train_dataset=emotion_encoded[\"train\"],\\r\\n                 eval_dataset=emotion_encoded[\"validation\"],\\r\\n                 tokenizer=tokenizer)\\r\\ntrainer.train()\\r\\n\\r\\n>and I am facing this error:\\r\\n>LocalTokenNotFoundError: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.\\r\\n>I have thought to apply the my token in the jupyter notebook like this:\\r\\n>```\\r\\n> ```python\\r\\n> from huggingface_hub import notebook_login\\r\\n> \\r\\n> notebook_login()\\r\\n>\\r\\n> ```\\r\\n>help me please:(\\r\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating text embeddings\n",
    "We can obtain token embeddings by using the AutoModel class. All we need to do is pick a suitable checkpoint to load the model from. Fortunately, there’s a library called sentence-transformers that is dedicated to creating embeddings. As described in the library’s [documentation](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search), our use case is an example of asymmetric semantic search because we have a short query whose answer we’d like to find in a longer document, like a an issue comment. The handy [model overview](https://www.sbert.net/docs/pretrained_models.html#model-overview) table in the documentation indicates that the multi-qa-mpnet-base-dot-v1 checkpoint has the best performance for semantic search, so we’ll use that for our application. We’ll also load the tokenizer using the same checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model checkpoint for multi-qa-mpnet-base-dot-v1\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "# Initialize the tokenizer using the pre-trained model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# Initialize the model using the pre-trained model checkpoint\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the embedding process, it helps to place the model and inputs on a GPU device, so let’s do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’d like to represent each entry in our GitHub issues corpus as a single vector, so we need to “pool” or average our token embeddings in some way. One popular approach is to perform CLS pooling on our model’s outputs, where we simply collect the last hidden state for the special [CLS] token. The following function does the trick for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function works by feeding it the first text entry in our corpus and inspecting the output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we’ve converted the first entry in our corpus into a 768-dimensional vector! We can use Dataset.map() to apply our get_embeddings() function to each row in our corpus, so let’s create a new embeddings column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46959/46959 [16:17<00:00, 48.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we’ve converted the embeddings to NumPy arrays — that’s because 🤗 Datasets requires this format when we try to index them with FAISS, which we’ll do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings dataset to disk\n",
    "embeddings_dataset.save_to_disk('embeddings_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FAISS for efficient similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset of embeddings, we need some way to search over them. To do this, we’ll use a special data structure in 🤗 Datasets called a [FAISS](https://faiss.ai/) index. FAISS (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors.\n",
    "\n",
    "The basic idea behind FAISS is to create a special data structure called an index that allows one to find which embeddings are similar to an input embedding. Creating a FAISS index in 🤗 Datasets is simple — we use the Dataset.add_faiss_index() function and specify which column of our dataset we’d like to index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings dataset from disk\n",
    "embeddings_dataset=datasets.load_from_disk('embeddings_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add faiss index\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform queries on this index by doing a nearest neighbor lookup with the Dataset.get_nearest_examples() function. Let’s test this out by first embedding a question as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the question\n",
    "question = \"How can I load a dataset offline?\"\n",
    "\n",
    "# Get the embeddings for the question\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "\n",
    "# Print the shape of the question embedding\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the documents, we now have a 768-dimensional vector representing the query, which we can compare against the whole corpus to find the most similar embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the nearest examples using embeddings dataset\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset.get_nearest_examples() function returns a tuple of scores that rank the overlap between the query and the document, and a corresponding set of samples (here, the 5 best matches). Let’s collect these in a pandas.DataFrame so we can easily sort them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert samples dictionary to DataFrame and sort by scores\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over the first few rows to see how well our query matched the available comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: The relevant docs to load from local data can be found here: https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained.\n",
      "\n",
      "The `from_pretrained` method accepts either a `repo_id` to a repo on the 🤗 hub or a local path to a folder.\n",
      "SCORE: 43.051761627197266\n",
      "TITLE: Provide a different API solution instead of offline mode\n",
      "URL: https://github.com/huggingface/transformers/issues/23117\n",
      "==================================================\n",
      "\n",
      "COMMENT: Yes, if you place all files which you find on the model page on the hub in a directory, then it will work.\n",
      "SCORE: 42.095550537109375\n",
      "TITLE: Getting a model to work on a system with no internet access\n",
      "URL: https://github.com/huggingface/transformers/issues/10900\n",
      "==================================================\n",
      "\n",
      "COMMENT: > You can do it, instead of loading `from_pretrained(roberta.large)` like this download the respective `config.json` and `<mode_name>.bin` and save it on your folder then just write `.from_pretrained('Users/<location>/<your folder name>')` and thats about it.\n",
      "\n",
      "This approach worked for facebook/m2m100_418M only after I downloaded every file at https://huggingface.co/facebook/m2m100_418M/tree/main except .gitattributes and README.md. (I can't swear that every single one is required, but I leave trial and error to determine the minimal set as an exercise for the reader.)\n",
      "SCORE: 37.597328186035156\n",
      "TITLE: How do I load a pretrained file offline?\n",
      "URL: https://github.com/huggingface/transformers/issues/2041\n",
      "==================================================\n",
      "\n",
      "COMMENT: @shashankMadan-designEsthetics' solution may require git-lfs to download the files of some models. If you are not a sudoer, this can be a problem. The most reliable and easy solution I've found is this:\n",
      "```\n",
      "from transformers import AutoModel, AutoTokenizer\n",
      "\n",
      "# Do this on a machine with internet access\n",
      "model = AutoModel.from_pretrained(\"model-name\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"model-name\")\n",
      "\n",
      "_ = model.save_pretrained(\"./model-dir\")\n",
      "_ = tokenizer.save_pretrained(\"./model-dir\")\n",
      "```\n",
      "Then you can do whatever you want with your model -- send it to a computing cluster, put it on a flash drive etc. Then you just do:\n",
      "```\n",
      "model = AutoModel.from_pretrained(\"path/model-dir\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"path/model-dir\")\n",
      "```\n",
      "SCORE: 37.320159912109375\n",
      "TITLE: How do I load a pretrained file offline?\n",
      "URL: https://github.com/huggingface/transformers/issues/2041\n",
      "==================================================\n",
      "\n",
      "COMMENT: You can do it, instead of loading `from_pretrained(roberta.large)` like this download the respective `config.json` and `<mode_name>.bin` and save it on your folder then just write \n",
      "`.from_pretrained('Users/<location>/<your folder name>')` and thats about it.\n",
      "SCORE: 36.16460418701172\n",
      "TITLE: How do I load a pretrained file offline?\n",
      "URL: https://github.com/huggingface/transformers/issues/2041\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! All the resulting outputs are helpful though the first two match the query the best. Let's try another query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Hi! I've used this:\n",
      "\n",
      "\n",
      "distilbert-base-uncased-distilled-squad |  \n",
      "-- | --\n",
      "or \n",
      "\n",
      "distilbert-base-cased-distilled-squad |  \n",
      "-- | --\n",
      "\n",
      "It improved quite a bit! \n",
      "\n",
      "SCORE: 32.76213836669922\n",
      "TITLE: How to speed up inference step in BertQuestionAnswering?\n",
      "URL: https://github.com/huggingface/transformers/issues/4535\n",
      "==================================================\n",
      "\n",
      "COMMENT: Sorry maybe I was not precise enough:\n",
      "Which model of the `transformers` library (e.g. Bert, GPT2) did you use? And can you copy / paste the exact code which has a `transformers` model in it that was slow for inference.\n",
      "SCORE: 32.67070007324219\n",
      "TITLE: How to speed up the transformer inference?\n",
      "URL: https://github.com/huggingface/transformers/issues/3753\n",
      "==================================================\n",
      "\n",
      "COMMENT: > I suggest you use Stack Overflow, where you will more likely receive answers to your question.\n",
      "\n",
      "OK, thx\n",
      "SCORE: 32.13627624511719\n",
      "TITLE: How to speed up the transformer inference?\n",
      "URL: https://github.com/huggingface/transformers/issues/3753\n",
      "==================================================\n",
      "\n",
      "COMMENT: Hi @hahadashi, \n",
      "\n",
      "Can you add a code snippet so that we know which model you are using and so that we can reproduce the behavior? \n",
      "SCORE: 31.987483978271484\n",
      "TITLE: How to speed up the transformer inference?\n",
      "URL: https://github.com/huggingface/transformers/issues/3753\n",
      "==================================================\n",
      "\n",
      "COMMENT: To improve inference speed you can use ONNX (also see here: https://github.com/huggingface/transformers/issues/260). In addition, you can opt for a distilled model rather than the full model. \n",
      "SCORE: 31.693700790405273\n",
      "TITLE: How to speed up inference step in BertQuestionAnswering?\n",
      "URL: https://github.com/huggingface/transformers/issues/4535\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding for the question\n",
    "question = \"How do I speed up inference?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "\n",
    "# Get the nearest examples from the embeddings dataset\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\"embeddings\", question_embedding, k=5)\n",
    "\n",
    "# Convert the samples to a DataFrame and add the scores\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "# Print the information for each sample\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The last result answered our query. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
